\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Background and Motivation}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Machine Learning Task and Objective}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Data Collection and Description}{2}{section.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Class distribution of mispriced versus non-mispriced snapshots; only about 3.7\% are labeled mispriced.}}{2}{figure.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Preprocessing}{3}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Models and Training Methodology}{3}{section.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Model characteristics.}}{3}{table.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Learning curve for logistic regression. Validation F1 flattens quickly, indicating limited capacity for nonlinear structure.}}{3}{figure.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Learning curve for random forest. Nonlinear interactions help, but gains taper as data increase.}}{4}{figure.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Learning curve for tuned XGBoost. Training and validation F1 rise together, suggesting better generalization without severe overfitting.}}{4}{figure.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Training and inference time (\texttt  {model\_runtime\_comparison.csv}).}}{4}{table.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Metrics}{5}{section.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Held-out metrics (\texttt  {model\_metrics\_comparison.csv}).}}{5}{table.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Results and Model Comparison}{5}{section.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces ROC curve for tuned XGBoost; ROC-AUC is about 0.94, meaning true mispricings generally score higher than normal cases.}}{5}{figure.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Precisionâ€“Recall curve; PR-AUC around 0.57 shows better-than-chance precision even as recall rises under heavy imbalance.}}{6}{figure.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Threshold sweep. Lower thresholds catch more mispricings with more false alarms; higher thresholds are more selective and precise.}}{6}{figure.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Confusion matrices for \(t=0.50\) (left) and \(t=0.70\) (right). Higher thresholds reduce false positives but miss more true mispricings.}}{7}{figure.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Calibration curve. Mid-range probabilities are conservative; a calibration step could refine them for trading.}}{8}{figure.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Cost-sensitive expected cost versus threshold. The preferred threshold shifts with the relative cost of misses versus false alarms.}}{9}{figure.10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Model Interpretation}{9}{section.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Top XGBoost feature importances. Volatility, order-flow pressure, and time to resolution are key drivers.}}{10}{figure.11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces SHAP summary. Wide spreads indicate strong influence; volatility and order-flow asymmetry tend to push predictions toward mispriced.}}{11}{figure.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces SHAP dependence for fifteen-minute sell volume. Sharp bursts of selling, especially near resolution, increase mispricing risk.}}{12}{figure.13}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Backtesting Analysis}{12}{section.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Per-trade PnL distributions. The 0.70 strategy trades less often but with a tighter, more favorable distribution; the 0.50 strategy trades more frequently with a broader spread.}}{13}{figure.14}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Total PnL and trade counts. The conservative threshold sacrifices volume for precision; the balanced threshold captures more opportunities with more noise.}}{13}{figure.15}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}Conclusion}{13}{section.10}\protected@file@percent }
\citation{*}
\bibstyle{plain}
\bibdata{references}
\bibcite{polymarket_api}{1}
\bibcite{polymarket_docs}{2}
\bibcite{xgboost}{3}
\bibcite{sklearn}{4}
\bibcite{matplotlib}{5}
\bibcite{shap}{6}
\bibcite{chatgpt}{7}
\bibcite{codex}{8}
\bibcite{pandas}{9}
\bibcite{seaborn}{10}
\gdef \@abspage@last{14}
